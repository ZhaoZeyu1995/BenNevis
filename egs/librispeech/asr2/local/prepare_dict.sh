#!/usr/bin/env bash
# Prepares two sentencepiece-based dictionaries for LibriSpeech,
# one for training and the other for evaluation.
# The training dictionary contains all the words in the training and dev sets,
# while the evaluation dictionary contains all the words in the official vocabulary
# plus the words in the training sets.
# We do so because during training, the training graph is constructed on-the-fly
# and a bigger vocabulary means a bigger lexicon FST, which is not computationally efficient.
# Usually, this is not an issue for small toy datasets, like timit and yesno.
# However, for large datasets, like LibriSpeech, it is better to use a smaller lexicon FST for training.
# Authors:
#   * Zeyu Zhao (The University of Edinburgh) 2024

. ./path.sh || exit 1

echo "$0 $@"  # Print the command line for logging

mtype=  # bpe or unigram
mtokens=  # number of tokens

. ./utils/parse_options.sh || exit 1

if [ $# != 3 ]; then
  echo "Usage: prepare_dict.sh <lm_dir> <dst_dir> <dst_eval_dir>"
  echo "e.g.:"
  echo " $0 data/local/lm data/local/dict_train data/local/dict"
  echo " where <lm_dir> is the directory containing the language model (generated by local/download_lm.sh),"
  echo " <dst_dir> is the directory to store the training dictionary,"
  echo " and <dst_eval_dir> is the directory to store the evaluation dictionary."
  exit 1
fi

if [ -z $mtype ]; then
    echo "$0: --mtype is not set"
    exit 1
fi

if [ -z $mtokens ]; then
    echo "$0: --mtokens is not set"
    exit 1
fi

lm_dir=$1
dst_dir=$2
dst_eval_dir=$3
tmpdir=data/local/dict_${mtype}_${mtokens}_tmp

# make sure that spm_train and spm_encode are installed and available in $PATH
which spm_train 1>/dev/null 2>/dev/null || \
    { echo "spm_train is not found. Please install sentencepiece (https://github.com/google/sentencepiece?tab=readme-ov-file#installation) and ensure it is in your PATH."; exit 1; }
which spm_encode 1>/dev/null 2>/dev/null || \
    { echo "spm_encode is not found. Please install sentencepiece (https://github.com/google/sentencepiece?tab=readme-ov-file#installation) and ensure it is in your PATH."; exit 1; }


[ -d $tmpdir ] && rm -rf $tmpdir
mkdir -p $tmpdir
[ -d $dst_dir ] && rm -rf $dst_dir
mkdir -p $dst_dir
[ -d $dst_eval_dir ] && rm -rf $dst_eval_dir
mkdir -p $dst_eval_dir

vocab=$lm_dir/librispeech-vocab.txt
[ ! -f $vocab ] && echo "$0: vocabulary file not found at $vocab" && exit 1;

for x in train_clean_100 train_clean_360 train_other_500; do
    [ ! -f data/$x/text ] && echo "$0: expected file data/$x/text to exist" && exit 1;
done

for x in train_clean_100 train_clean_360 train_other_500; do
    cat data/$x/text 
done > $tmpdir/text.train

cat $tmpdir/text.train | cut -d' ' -f2- | tr ' ' '\n' | grep -v -w '<UNK>' | grep -v -w '<SIL>' | sort | uniq > $tmpdir/words.train

for x in dev_clean dev_other; do
    [ ! -f data/$x/text ] && echo "$0: expected file data/$x/text to exist" && exit 1;
    cat data/$x/text | cut -d' ' -f2- | tr ' ' '\n' 
done | grep -v -w '<UNK>' | grep -v -w '<SIL>' | sort | uniq > $tmpdir/words.dev
cat $vocab | sort | uniq > $tmpdir/words.vocab

cat $tmpdir/words.train $tmpdir/words.vocab | sort | uniq > $tmpdir/words.eval
cat $tmpdir/words.train $tmpdir/words.dev | sort | uniq > $tmpdir/words.train_dev

# Prepare the dictionary for evaluation
echo "$0: Preparing the dictionary in $dst_eval_dir for evaluation"

echo "<SIL>" > $dst_eval_dir/optional_silence.txt
echo "<SIL>" > $dst_eval_dir/silence_phones.txt
echo "<SIL> <SIL>" > $dst_eval_dir/lexicon.txt
echo "<UNK> <UNK>" >> $dst_eval_dir/lexicon.txt
echo "<UNK>" > $dst_eval_dir/nonsilence_phones.txt
# get sentencepiece dictionary
spm_train --input=$tmpdir/text.train --model_prefix=$tmpdir/${mtype}_${mtokens} --vocab_size=${mtokens} --character_coverage=1.0 --model_type=${mtype}
spm_encode --model=$tmpdir/${mtype}_${mtokens}.model --output_format=piece < $tmpdir/words.eval > $tmpdir/raw.lexicon.txt
paste -d ' ' $tmpdir/words.eval $tmpdir/raw.lexicon.txt | sort | uniq >> $dst_eval_dir/lexicon.txt
cat $tmpdir/${mtype}_${mtokens}.vocab | awk '{print $1}'| grep -v -w "<unk>" | grep -v -w "<s>" | grep -v -w "</s>" | awk 'NF>0' >> $dst_eval_dir/nonsilence_phones.txt

echo "$0: Done"

# Prepare the dictionary for training
echo "$0: Preparing the dictionary in $dst_dir for training"

cp $dst_eval_dir/optional_silence.txt $dst_dir/optional_silence.txt
cp $dst_eval_dir/silence_phones.txt $dst_dir/silence_phones.txt
cp $dst_eval_dir/nonsilence_phones.txt $dst_dir/nonsilence_phones.txt

echo "<SIL> <SIL>" > $dst_dir/lexicon.txt
echo "<UNK> <UNK>" >> $dst_dir/lexicon.txt
# get character-level dictionary
spm_encode --model=$tmpdir/${mtype}_${mtokens}.model --output_format=piece < $tmpdir/words.train_dev > $tmpdir/raw.lexicon.train_dev.txt
paste -d ' ' $tmpdir/words.train_dev $tmpdir/raw.lexicon.train_dev.txt | sort | uniq >> $dst_dir/lexicon.txt

echo "$0: Done"
exit 0
